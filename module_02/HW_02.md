---
jupytext:
  formats: notebooks//ipynb,md:myst
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.11.4
kernelspec:
  display_name: Python 3 (ipykernel)
  language: python
  name: python3
---

> __Content modified under Creative Commons Attribution license CC-BY
> 4.0, code under BSD 3-Clause License Â© 2020 R.C. Cooper__

+++

# Homework 2 By: Zyaja Huggan

```{code-cell} ipython3
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')
```

## Problems [Part 1](./01_Cheers_Stats_Beers.md)

1. Gordon Moore created an empirical prediction that the rate of
semiconductors on a computer chip would double every two years. This
prediction was known as Moore's law. Gordon Moore had originally only
expected this empirical relation to hold from 1965 - 1975
[[1](https://en.wikipedia.org/wiki/Moore%27s_law),[2](https://spectrum.ieee.org/computing/hardware/gordon-moore-the-man-whose-name-means-progress)],
but semiconductor manufacturers were able to keep up with Moore's law
until 2015. 

In the folder "../data" is a comma separated value (CSV) file,
"transistor_data.csv" [taken from wikipedia
01/2020](https://en.wikipedia.org/wiki/Transistor_count#Microprocessors).

a. Use the `!head ../data/transistor_data.csv` command to look at
the top of the csv. What are the headings for the columns?

b. Load the csv into a pandas dataframe. How many missing values
(`NaN`) are
in the column with the number of transistors? What fraction are
missing?

```{code-cell} ipython3
#1a) 
!head ../data/transistor_data.csv #use !head command to look into the data file
```

The headings for the columns are Processor, MOS transistor count, Date of Introduction, Designer, MOSprocess, and Area.

```{code-cell} ipython3
#1b)
data_file = pd.read_csv('../data/transistor_data.csv') #load csv in 
missing_values_NaN = data_file['MOS transistor count']
length_all = len(missing_values_NaN) #length of all vaues
missing_clean = missing_values_NaN.dropna() #gets all the good values
length_clean = len(missing_clean) #length of good values
num_missing_values = length_all - length_clean #by subtracting all the good values from the total it will leave with the missing values
missing_fraction = (num_missing_values/length_all)*100 #percentage of the missing values over all the values
data_file
print(num_missing_values)
print(missing_fraction)
```

```{code-cell} ipython3
data_file 
```

There are 7 missing values(NaN) in the column with the number of transistors. There is a 3.76% fraction of them missing.

+++

## Problems [Part 2](./02_Seeing_Stats.md)

1. Many beers do not report the IBU of the beer because it is very
small. You may be accidentally removing whole categories of beer from
our dataset by removing rows that do not include the IBU measure. 

    a. Use the command `beers_filled = beers.fillna(0)` to clean the `beers` dataframe
    
    b. Repeat the steps above to recreate the plot "Beer ABV vs. IBU mean values by style" 
    scatter plot with `beers_filled`. What differences do you notice between the plots?

```{code-cell} ipython3
#2.1a)
beers = pd.read_csv('../data/beers.csv') #read the csv file in 
beers
```

```{code-cell} ipython3
beers_filled = beers.fillna(0) #clean dataframe
#beers_clean = beers.dropna() #remove data
beers_styles = beers_filled.drop(['Unnamed: 0','name','brewery_id','ounces','id'], axis=1) #clear headings
style_counts = beers_styles['style'].value_counts()
style_means = beers_styles.groupby('style').mean()
style_counts = style_counts.sort_index()
ibu = beers_filled['ibu'].values #clean ibu values
abv = beers_filled['abv'].values #clean abv values
```

```{code-cell} ipython3
#2.1b)
from matplotlib import cm
colors = cm.viridis(style_counts.values)
```

# Revision 1 - Use Beers Filled

```{code-cell} ipython3
#using information learned from the workbooks
ax = style_means.plot.scatter(figsize=(10,10), 
                               x='abv', y='ibu', s=style_counts*20, color=colors,
                               title='Beer ABV vs. IBU mean values by style\n',
                               alpha=0.3);

for i, txt in enumerate(list(style_counts.index.values)):
    if style_counts.values[i] > 65:
        ax.annotate(txt, (style_means.abv.iloc[i],style_means.ibu.iloc[i]), fontsize=12)
```

I notice that much of the data is in the bottom left quadrant of the plot and it is a lot easier to visiualize the similairites in the data since they are more grouped together. The amount that both axis go up is also different since the data is more grouped together.

+++

2. Gordon Moore created an empirical prediction that the rate of
semiconductors on a computer chip would double every two years. This
prediction was known as Moore's law. Gordon Moore had originally only
expected this empirical relation to hold from 1965 - 1975
[[1](https://en.wikipedia.org/wiki/Moore%27s_law),[2](https://spectrum.ieee.org/computing/hardware/gordon-moore-the-man-whose-name-means-progress)],
but semiconductor manufacturers were able to keep up with Moore's law
until 2015. 

    In the folder "../data" is a comma separated value (CSV) file, "transistor_data.csv" [taken from wikipedia 01/2020](https://en.wikipedia.org/wiki/Transistor_count#Microprocessors). 
    Load the csv into a pandas dataframe, it has the following headings:

    |Processor| MOS transistor count| Date of Introduction|Designer|MOSprocess|Area|
    |---|---|---|---|---|---|

    a. In the years 2017, what was the average MOS transistor count? 
    Make a boxplot of the transistor count in 2017 and find the first, second and third quartiles.

    b. Create a semilog y-axis scatter plot (i.e. `plt.semilogy`) for the 
    "Date of Introduction" vs "MOS transistor count". 
    Color the data according to the "Designer".

```{code-cell} ipython3
#2.2a)
data_2017 = data_file[data_file['Date of Introduction']==2017] #refer to the 2017 data
transistor_count_2017 = data_2017['MOS transistor count'] #refer to the MOS transistor count column of the 2017 data
```

```{code-cell} ipython3
transistor_count_mean_2017 = data_2017['MOS transistor count'].mean() #mean of the data
print(transistor_count_mean_2017)
```

The mean of the data is 7050000000.0.

```{code-cell} ipython3
plt.boxplot(transistor_count_2017, labels=['Transistor Count in 2017']) #makes boxplot of data
```

```{code-cell} ipython3
quartiles = np.percentile(transistor_count_2017, q=[25, 50, 75]) #uses list comprehension to refer to the quartiles

print('The first quartile for the transistor count in 2017 is {}'.format(quartiles[0]))
print('The second quartile for the transistor count in 2017 is {}'.format(quartiles[1]))
print('The third quartile for the transistor count in 2017 is {}'.format(quartiles[2]))
```

```{code-cell} ipython3
#2.2b)
for name in data_file['Designer'].unique(): #refer to designer column and find unique elements
    data = data_file[data_file['Designer'] == name]  #refer to the names of each 
    plt.semilogy(data['Date of Introduction'],data['MOS transistor count'], 'o', label= name) #plot the date and transistor count of each
    plt.title('Date of Introduction vs MOS transistor count')
plt.legend(bbox_to_anchor=(1,1)) #adds labels and move labels over so that graph can be seen 
```

## Problems [Part 3](03_Linear_Regression_with_Real_Data.md)

1. There is a csv file in '../data/primary-energy-consumption-by-region.csv' that has the energy consumption of different regions of the world from 1965 until 2018 [Our world in Data](https://ourworldindata.org/energy). 
Compare the energy consumption of the United States to all of Europe. Load the data into a pandas dataframe. *Note: you can get certain rows of the data frame by specifying what you're looking for e.g. 
`EUR = dataframe[dataframe['Entity']=='Europe']` will give us all the rows from Europe's energy consumption.*

    a. Plot the total energy consumption of the United States and Europe
    
    b. Use a linear least-squares regression to find a function for the energy consumption as a function of year
    
    energy consumed = $f(t) = At+B$
    
    c. At what year would you change split the data and use two lines like you did in the 
    land temperature anomoly? Split the data and perform two linear fits. 
    
    d. What is your prediction for US energy use in 2025? How about European energy use in 2025?

```{code-cell} ipython3
#3.1a)
energy = pd.read_csv('../data/primary-energy-consumption-by-region.csv')
energy
```

```{code-cell} ipython3
us_entity = energy[energy['Entity'] == 'United States'] #refer to United States column
europe_entity = energy[energy['Entity'] == 'Europe'] #refer to Europe column 
```

```{code-cell} ipython3
plt.plot(us_entity['Year'], us_entity['Primary Energy Consumption (terawatt-hours)'], label='Energy US') #plot year and energy consumption for us
plt.plot(europe_entity['Year'], europe_entity['Primary Energy Consumption (terawatt-hours)'], label='Energy Europe') #plot year and energy consumption for europe
plt.xlabel('Year') #xaxis is year
plt.ylabel('Primary Energy Consumption (terawatt-hours)') #yaxis is energy consumption
plt.title('Total Energy Consumption US vs. Europe') #title of plot
plt.legend() #labels 
```

```{code-cell} ipython3
#3.1b)
us_xaxis = us_entity['Year'].values #refer to the us year values
europe_xaxis = europe_entity['Year'].values #refer to europe year values

us_yaxis = us_entity['Primary Energy Consumption (terawatt-hours)'].values #refer to us energy consumption
europe_yaxis = europe_entity['Primary Energy Consumption (terawatt-hours)'].values #refer to europe energy consumption

us_xaxis_mean = np.mean(us_xaxis) #finds mean of us year values - xaxis
europe_xaxis_mean = np.mean(europe_xaxis) #finds mean of europe year values - xaxis

us_yaxis_mean = np.mean(us_yaxis) #finds mean of us year values - yaxis
europe_yaxis_mean = np.mean(europe_yaxis) #finds mean of europe year values - yaxis

def coefficients(x, y, x_mean, y_mean):
    """
    Determines coefficients 
    ---------
    x: independent variable 
    y: dependent variable measurements
    x_mean: mean of independent variable
    y_mean: mean of dependent variable
    Returns
    -------
    a_1:Â the least-squares regression slope
    a_0: the least-squares regression intercept
    """
    a_1 = np.sum(y*(x - x_mean)) / np.sum(x*(x - x_mean)) #equation for least-square regression slope
    a_0 = y_mean - a_1*x_mean #equation for least-square regression intercept
    
    return a_1, a_0
```

```{code-cell} ipython3
us_coeff = coefficients(us_xaxis, us_yaxis, us_xaxis_mean, us_yaxis_mean) #finds us coefficients
europe_coeff = coefficients(europe_xaxis, europe_yaxis, europe_xaxis_mean, europe_yaxis_mean) #finds europe coefficients

us_a_1, us_a_0 = us_coeff #tuple unpacking to refer to a1 and a0
us_reg = us_a_0 + us_a_1 * us_xaxis #regression for us

europe_a_1, europe_a_0 = europe_coeff
europe_reg = europe_a_0 + europe_a_1 * europe_xaxis #regression for europe
```

```{code-cell} ipython3
plt.figure(figsize=(10, 5))
plt.plot(us_entity['Year'], us_entity['Primary Energy Consumption (terawatt-hours)'], label = 'United States')  #plot us
plt.plot(europe_entity['Year'], europe_entity['Primary Energy Consumption (terawatt-hours)'], label = 'Europe') #plot europe
plt.plot(us_xaxis, us_reg, 's', label = 'US Linear Regression') #linear regression us
plt.plot(europe_xaxis, europe_reg, 'k--', label = 'Europe Linear Regression') #linear regression europe
plt.title('Total Energy Consumption US vs. Europe') #title
plt.xlabel('Year') #xaxis label
plt.ylabel('Primary Energy Consumption (terawatt-hours)') #y-axis label
plt.legend() #labels
```

```{code-cell} ipython3
us_linear = np.poly1d((europe_a_1, europe_a_0)) #linear fir for us
europe_linear=np.poly1d((us_a_1, us_a_0)) #linear fit for europe
print(us_linear)
print(europe_linear)
```

# Revision 2 - Using two splits instead of 3 and how many TW in 2025

+++

I originally had 3 splits because I felt like the trends shifts at two points but I realize it might not be a big enough shift to have another split. Below I have altered there to just be two splits.

+++

The function for United States energy consumed is f(t) = 199.6 x - 3.76e+05.
The function for Europe energy consumed is f(t) = 200.4 x - 3.765e+05.

```{code-cell} ipython3
#3.1c)
#United States
a_1n, a_0n = np.polyfit(us_xaxis, us_yaxis, 1)
f_linear = np.poly1d((a_1n, a_0n)) #one dimension polynomials
f_linear = lambda x: a_1n*x+a_0n #one dimension polynomials

plt.figure(figsize=(10, 5))

plt.plot(us_xaxis, us_yaxis,'s', color='#2929a3', linewidth=1, alpha=0.5,label='Energy Consumption') #us energy consumption
plt.plot(us_xaxis, f_linear(us_xaxis), 'k--', linewidth=2, label='Linear regression') #us linear regression
plt.title('Energy Consumption vs Year(US)')
plt.xlabel('Year')
plt.ylabel('Energy Concumption')
plt.legend(loc='best', fontsize=15)
plt.grid();
```

```{code-cell} ipython3
x = us_entity['Year'].values #year
y = us_entity['Primary Energy Consumption (terawatt-hours)'].values #energy consumption for us
#mean of year and energy consumption for us
mean_x = np.mean(x) 
mean_y = np.mean(y)
year = x
energy_consumed = y
```

I would split the data at 1970 since that is where the trends shift either increasing or decreasing. I would do one line from 1965 to 1969, 1970 to 2020.

```{code-cell} ipython3
#splitting the data and one dimensional polynomials
year_1 , energy_consumed_1 = year[0:5], energy_consumed[0:5] #'1965-1969'
year_2 , energy_consumed_2 = year[5:], energy_consumed[5:] #'1970-2020'
#year_3 , energy_consumed_3 = year[45:], energy_consumed[45:] #'2010-2020'

m1, b1 = np.polyfit(year_1, energy_consumed_1, 1)
m2, b2 = np.polyfit(year_2, energy_consumed_2, 1)
#m3, b3 = np.polyfit(year_3, energy_consumed_3, 1)

f_linear_1 = np.poly1d((m1, b1))
f_linear_2 = np.poly1d((m2, b2))
#f_linear_3 = np.poly1d((m3, b3))
```

```{code-cell} ipython3
plt.figure(figsize=(10, 5))

plt.plot(year, energy_consumed, color='#2929a3', linestyle='-', linewidth=1, alpha=0.5) 
plt.plot(year_1, f_linear_1(year_1), 'g--', linewidth=2, label='1965-1969') #plot first split
plt.plot(year_2, f_linear_2(year_2), 'r--', linewidth=2, label='1970-2009') #plot second split
#plt.plot(year_3, f_linear_3(year_3), 'b--', linewidth=2, label='2010-2020') #plot thrid split

plt.xlabel('Year')
plt.ylabel('Primary Energy Consumption (terawatt-hours)')
plt.title('Split Regression for Europe')
plt.legend(loc='best', fontsize=15)
plt.grid();
```

```{code-cell} ipython3
#Europe
a_1n, a_0n = np.polyfit(europe_xaxis, europe_yaxis, 1)
f_linear = np.poly1d((a_1n, a_0n)) 
f_linear = lambda x: a_1n*x+a_0n

plt.figure(figsize=(10, 5))

plt.plot(europe_xaxis, europe_yaxis,'s', color='#2929a3', linewidth=1, alpha=0.5,label='Energy Consumption')
plt.plot(europe_xaxis, f_linear(europe_xaxis), 'k--', linewidth=2, label='Linear regression')
plt.title('Energy Consumption vs Year(Europe)')
plt.xlabel('Year')
plt.ylabel('Energy Consumption')
plt.legend(loc='best', fontsize=15)
plt.grid();
```

```{code-cell} ipython3
x = europe_entity['Year'].values
y = europe_entity['Primary Energy Consumption (terawatt-hours)'].values
mean_x = np.mean(x)
mean_y = np.mean(y)
year = x
energy_consumed = y
```

I would split the data at 1984 because the trends shift close to those years either increasing or decreasing. I would do one line from 1965 to 1984, 1985 to 2020.

```{code-cell} ipython3
#splitting the data and one dimensional polynomials
year_1 , energy_consumed_1 = year[0:20], energy_consumed[0:20] #'1965-1984'
year_2 , energy_consumed_2 = year[20:], energy_consumed[20:] #'1985-2009'
#year_3 , energy_consumed_3 = year[45:], energy_consumed[45:] #'2010-2020'

m1, b1 = np.polyfit(year_1, energy_consumed_1, 1)
m2, b2 = np.polyfit(year_2, energy_consumed_2, 1)
#m3, b3 = np.polyfit(year_3, energy_consumed_3, 1)

f_linear_1 = np.poly1d((m1, b1))
f_linear_2 = np.poly1d((m2, b2))
#f_linear_3 = np.poly1d((m3, b3))
```

plt.figure(figsize=(10, 5))

plt.plot(year, energy_consumed, color='#2929a3', linestyle='-', linewidth=1, alpha=0.5) 
plt.plot(year_1, f_linear_1(year_1), 'g--', linewidth=2, label='1965-1984') #plot first split
plt.plot(year_2, f_linear_2(year_2), 'r--', linewidth=2, label='1985-2009') #plot second split
#plt.plot(year_3, f_linear_3(year_3), 'b--', linewidth=2, label='2010-2020') #plot third split

plt.xlabel('Year')
plt.ylabel('Primary Energy Consumption (terawatt-hours)')
plt.title('Split Regression for Europe')
plt.legend(loc='best', fontsize=15)
plt.grid();

+++

# TW in 2025

+++

3.1d) Based on the trends indicated above, I expect that the US energy use would keep increasing as the blue line is increasing and the Europe energy use increase slightly as well. The US would increase a lot more than europe based on the trends I see. In 2025, I would expect the TW for the US to be about 28000 and for Europe to be about 25000.

+++

2. You plotted Gordon Moore's empirical prediction that the rate of semiconductors on a computer chip would double every two years in [02_Seeing_Stats](./02_Seeing_Stats). This prediction was known as Moore's law. Gordon Moore had originally only expected this empirical relation to hold from 1965 - 1975 [[1](https://en.wikipedia.org/wiki/Moore%27s_law),[2](https://spectrum.ieee.org/computing/hardware/gordon-moore-the-man-whose-name-means-progress)], but semiconductor manufacuturers were able to keep up with Moore's law until 2015. 

Use a linear regression to find our own historical Moore's Law.    

Use your code from [02_Seeing_Stats](./02_Seeing_Stats) to plot the semilog y-axis scatter plot 
(i.e. `plt.semilogy`) for the "Date of Introduction" vs "MOS transistor count". 
Color the data according to the "Designer".

Create a linear regression for the data in the form of 

$log(transistor~count)= f(date) = A\cdot date+B$

rearranging

$transistor~count= e^{f(date)} = e^B e^{A\cdot date}$

You can perform a least-squares linear regression using the following assignments

$x_i=$ `dataframe['Date of Introduction'].values`

and

$y_i=$ as `np.log(dataframe['MOS transistor count'].values)`

a. Plot your function on the semilog y-axis scatter plot

b. What are the values of constants $A$ and $B$ for our Moore's law fit? How does this compare to Gordon Moore's prediction that MOS transistor count doubles every two years?

```{code-cell} ipython3
#3.2a)
data = pd.read_csv('../data/transistor_data.csv')
data
```

```{code-cell} ipython3
data = data.dropna() #clear data
data_file = data.drop(['Processor','MOSprocess','Area'], axis=1) #clear headings
xi = data_file['Date of Introduction'].values
TC = np.log(data_file['MOS transistor count'].values)

for name in data_file['Designer'].unique(): #refer to designer column and find unique elements
    data_values = data_file[data_file['Designer'] == name]  #refer to the names of each 
    plt.semilogy(data_values['Date of Introduction'],data_values['MOS transistor count'], 'o', label= name) #plot the date and transistor count of each
plt.title('Date of Introduction vs MOS transistor count')
plt.legend(bbox_to_anchor=(1,1)) #adds labels and move labels over so that graph can be seen 
```

```{code-cell} ipython3
#3.2b)
a_1n, a_0n = np.polyfit(xi, TC, 1) 
f_linear = np.poly1d((a_1n, a_0n)) #one dimensional polynomial
plt.plot(xi, np.exp(f_linear(xi)), label='Linear regression')
```

```{code-cell} ipython3
a_1n, a_0n = np.polyfit(xi, TC, 1)
f_linear = np.poly1d((a_1n, a_0n))
plt.plot(xi, np.exp(f_linear(xi)), label='Linear regression')


for name in data_file['Designer'].unique(): #refer to designer column and find unique elements
    data_values = data_file[data_file['Designer'] == name]  #refer to the names of each 
    plt.semilogy(data_values['Date of Introduction'],data_values['MOS transistor count'], 'o', label= name) #plot the date and transistor count of each
plt.title('Date of Introduction vs MOS transistor count')
plt.legend(bbox_to_anchor=(1,1)) #adds labels and move labels over so that graph can be seen 
print(f_linear)
```

# Revision 3 - What would a_1n be if it doubled?

+++

Past Answer: To answer this question I guess a_1n wouln't double based on the trend. If I take a look back at the linear regression it looks more exponential than quadratic so it would not meet the prediction.

+++

Past Answer: The values of constant A and B are 0.3359 and -654.7 respectively. Gordon Moore's prediction that MOS transistor count doubles every two years seems to be correct since the best fit line fits the data. However, I was a little confused here because when I plot the linear regression seperately it looks more like a quadratic trend but when I plot it on the actual graph it matches. I am assuming the data matches a small portion of the trend.

+++

# New Revision: Gordon Moore Prediction Analysis

+++

I have implemented your advice on how to look closer at the Gordon Moore prediction. This makes more sense to me now that I see both of the lines with the data. The linear regression and the Gordon Moore Prediciton seem to follow the same trend verifying that the prediciton is correct. The data would double with a linear trend.

```{code-cell} ipython3
#polyfit
a_1n, a_0n = np.polyfit(xi, TC, 1)
f_linear = np.poly1d((a_1n, a_0n))
plt.plot(xi, np.exp(f_linear(xi)), label='Linear regression')

#gordon moore
time = np.arange(1971, 2021, 2)
chips = np.zeros(len(time))
chips[0] = 2.25e3
for i in range(1, len(chips)):
    chips[i] = 2*chips[i-1]

plt.semilogy(time, chips, label = 'Moore')


for name in data_file['Designer'].unique(): #refer to designer column and find unique elements
    data_values = data_file[data_file['Designer'] == name]  #refer to the names of each 
    plt.semilogy(data_values['Date of Introduction'],data_values['MOS transistor count'], 'o', label= name) #plot the date and transistor count of each
plt.title('Date of Introduction vs MOS transistor count')
plt.legend(bbox_to_anchor=(1,1)) #adds labels and move labels over so that graph can be seen 
print(f_linear)
```

## Problems [Part 4](04_Stats_and_Montecarlo.md)

__1.__ [Buffon's needle problem](https://en.wikipedia.org/wiki/Buffon) is
another way to estimate the value of $\pi$ with random numbers. The goal
in this Monte Carlo estimate of $\pi$ is to create a ratio that is close
to [3.1415926...](http://www.math.com/tables/constants/pi.htm) _similar
to the example with darts points lying inside/outside a unit circle
inside a unit square._ 

![Buffon's needle for parallel
lines](https://upload.wikimedia.org/wikipedia/commons/f/f6/Buffon_needle.gif)

In this Monte Carlo estimation, you only need to know two values:
- the distance from line 0, $x = [0,~1]$
- the orientation of the needle, $\theta = [0,~2\pi]$

The y-location does not affect the outcome of crosses line 0 or not
crossing line 0. 

__a.__ Generate 100 random `x` and `theta` values _remember_ $\theta =
[0,~2\pi]$

__b.__ Calculate the x locations of the 100 needle ends e.g. $x_end = x
\pm \cos\theta$ _since length is unit 1. 

__c.__ Use 
[`np.logical_and`](https://numpy.org/doc/stable/reference/generated/numpy.logical_and.html)
to find the number of needles that have minimum $x_{end~min}<0$ and
maximum $x_{end~max}>0$. The ratio
$\frac{x_{end~min}<0~and~x_{end~max}>0}{number~of~needles} =
\frac{2}{\pi}$ _for large values of $number~of~needles$_.

__2.__ Build a random walk data set with steps between $dx = dy =
-1/2~to~1/2~m$. If 100 particles take 10 steps, calculate the number of
particles that move further than 0.5 m. 

_Bonus: Can you do the work without any `for`-loops? Change the size of
`dx` and `dy` to account for multiple particles._

```{code-cell} ipython3
#4.1a)
from numpy.random import default_rng
rng = default_rng()
random_numbers = 100
x = rng.random(100) #100 random numbers
theta = rng.random(100)*2*np.pi #100 random theta values that are between 0 and 2pi
print(x)
print(theta)
```

For 4.1b, I used an array to account for the plus/minus portions of the equation and the I used min and max to find 100 values below and above.

```{code-cell} ipython3
#4.1b)
end = x + np.array([np.cos(theta),-np.cos(theta)]) #equation ð‘¥ð‘’ð‘›ð‘‘=ð‘¥Â±cosðœƒ, used an array to account for the +/- 
minus = np.min(end, 0) #get the values below x using min
plus = np.max(end, 0) #get the values above x using max 
print(minus) #100 locations below x
print(plus) #100 locations above x
```

For this question(4.1c), I used np.logicial_and and put both the minimum and maximum ends. I was a little confused for a while on how to do what the question is aksing but below is what I think it is asking us to do. I summed up everythig in the array since it wouldn't give just one number and then I divided by the number of needles. The answer I got after doing that was 0.67. The answer when I do 2 over pi was about 0.64. These two numbers are very close in value.

+++

# Revision 4 - Drop the print statements and try increasing the number of needles

```{code-cell} ipython3
#4.1c)
random_numbers = 100
num_of_needles = np.sum(np.logical_and(end.max(axis = 0) > 0, end.min(axis = 0) < 0))/random_numbers #using np.logical_and to see how many True and Falses there are
actual = 2/np.pi
print(num_of_needles, actual)
```

```{code-cell} ipython3
random_numbers = 105
num_of_needles = np.sum(np.logical_and(end.max(axis = 0) > 0, end.min(axis = 0) < 0))/random_numbers #using np.logical_and to see how many True and Falses there are
actual = 2/np.pi
print(num_of_needles, actual)
```

I treid to experiment by increasing the number of needles like you mentioned and it look like it would bring me closer to the correct answer.

+++

For 4.2, I tried to follow the workbooks provided to us. I have made equaivalent arrays of zeros in order to build the data set.

```{code-cell} ipython3
#4.2)
num_of_particles = 100 
N_steps = 10
r_final = np.zeros((num_of_particles, 2)) #array of zeros
for i in range(0, num_of_particles):
    dx = rng.random(N_steps) - 0.5 #steps +/- 1/2
    dy = rng.random(N_steps) - 0.5

    r = np.zeros((N_steps, 2))

    r[:, 0] = np.cumsum(dx) #finds the cumalative sum
    r[:, 1] = np.cumsum(dy)
    r_final[i, :] = r[-1, :]
    plt.plot(r[:, 0 ], r[:, 1], alpha = 0.5)
plt.plot(r_final[:, 0], r_final[:, 1], 'o', markersize = 10)
```

```{code-cell} ipython3
distance_equation = np.sqrt((r_final[:,0])**2+(r_final[:,1])**2) #distance equation to find distance of all 
print(distance_equation)
counter = 0 #keeps track of how many are over 0.5
for i in distance_equation:
    if i > 0.5:
        counter += 1
print(counter)
```

The number of particles that move futher than 0.5m are 83 according to the distance equation.

+++

__3.__ 100 steel rods are going to be used to support a 1000 kg structure. The
rods will buckle when the load in any rod exceeds the [critical buckling
load](https://en.wikipedia.org/wiki/Euler%27s_critical_load)

$P_{cr}=\frac{\pi^3 Er^4}{16L^2}$

where E=200e9 Pa, r=0.01 m +/-0.001 m, and L is the 
length of the rods supporting the structure. Create a Monte
Carlo model `montecarlo_buckle` that predicts 
the mean and standard deviation of the buckling load for 100
samples with normally distributed dimensions r and L. 

```python
mean_buckle_load,std_buckle_load=\
montecarlo_buckle(E,r_mean,r_std,L,N=100)
```

__a.__ What is the mean_buckle_load and std_buckle_load for L=5 m?

__b.__ What length, L, should the beams be so that only 2.5% will 
reach the critical buckling load?

+++

# Revision 5 - Use a random set of values instead

```{code-cell} ipython3
#4.3a)
def montecarlo_buckle(E,r_mean,r_std,L,N=100):
    '''Generate N rods of length L with radii of r=r_mean+/-r_std
    then calculate the mean and std of the buckling loads in for the
    rod population holding a 1000-kg structure
    Arguments
    ---------
    E: Young's modulus [note: keep units consistent]
    r_mean: mean radius of the N rods holding the structure
    r_std: standard deviation of the N rods holding the structure
    L: length of the rods (or the height of the structure)
    N: number of rods holding the structure, default is N=100 rods
    Returns
    -------
    mean_buckle_load: mean buckling load of N rods under 1000*9.81/N-Newton load
    std_buckle_load: std dev buckling load of N rods under 1000*9.81/N-Newton load
    '''
    
    r = rng.normal(r_mean, r_std, N) #r=r_mean+/-r_std
    critical_buckling_load = ((np.pi**3)*(E)*(r**4))/((16)*(L)**2) #critical buckling load equation
    mean_buckle_load = np.mean(critical_buckling_load) #mean of critical buckling load
    std_buckle_load = np.std(critical_buckling_load) #standard deviation of critical buckling load 
    return (mean_buckle_load, std_buckle_load)
```

```{code-cell} ipython3
#plugging values into the function
E = 200*10**9
r_mean = 0.01
r_std = 0.001
L = 5
montecarlo_buckle(E,r_mean,r_std,L,N=100)
```

The mean buckling load is 169.03 and the standard deviation of the buckling load is 62.21.

```{code-cell} ipython3
#4.3b)
def montecarlo_bucklelength(E,r_mean,r_std,L,N=100):
    '''Generate N rods of length L with radii of r=r_mean+/-r_std
    then calculate the mean and std of the buckling loads in for the
    rod population holding a 1000-kg structure
    Arguments
    ---------
    E: Young's modulus [note: keep units consistent]
    r_mean: mean radius of the N rods holding the structure
    r_std: standard deviation of the N rods holding the structure
    L: length of the rods (or the height of the structure)
    N: number of rods holding the structure, default is N=100 rods
    Returns
    -------
    mean_buckle_load: mean buckling load of N rods under 1000*9.81/N-Newton load
    std_buckle_load: std dev buckling load of N rods under 1000*9.81/N-Newton load
    '''
    
    r = rng.normal(r_mean, r_std, N) #r=r_mean+/-r_std
    critical_buckling_load = ((np.pi**3)*(E)*(r**4))/((16)*(L)**2) #critical buckling load equation
    length = np.sqrt(((np.pi**3)*(E)*(r**4))/(16*(critical_buckling_load*0.025))) #solving for length taking into account the 2.5%
    return length
```

```{code-cell} ipython3
montecarlo_bucklelength(E,r_mean,r_std,L,N=100) #plugging values into function
```

# Revision 6 - Compare the Buckling Load in the Rods to the Applied Load

+++

The length, L, that the beams should be so that only 2.5% will reach the critical buckling load is about 31.62m.

```{code-cell} ipython3
def montecarlo_loads(E,r_mean,r_std,L,N=100):
    '''Generate N rods of length L with radii of r=r_mean+/-r_std
    then calculate the mean and std of the buckling loads in for the
    rod population holding a 1000-kg structure
    Arguments
    ---------
    E: Young's modulus [note: keep units consistent]
    r_mean: mean radius of the N rods holding the structure
    r_std: standard deviation of the N rods holding the structure
    L: length of the rods (or the height of the structure)
    N: number of rods holding the structure, default is N=100 rods
    Returns
    -------
    mean_buckle_load: mean buckling load of N rods under 1000*9.81/N-Newton load
    std_buckle_load: std dev buckling load of N rods under 1000*9.81/N-Newton load
    '''
    
    r = rng.normal(r_mean, r_std, N) #r=r_mean+/-r_std
    critical_buckling_load = ((np.pi**3)*(E)*(r**4))/((16)*(L)**2) #critical buckling load equation
    return critical_buckling_load
```

```{code-cell} ipython3
#applied load
F = (1000*9.81)/100
F
```

```{code-cell} ipython3
#load with new length 
#comparison method 1
m, s = montecarlo_buckle(200e9,0.01,0.001,L = 31.62,N=100000)
m-2*s #not close to the applied load 
```

```{code-cell} ipython3
#comparison method 2
Nsim = 100000
critical_buckling_load = montecarlo_loads(200e9,0.01,0.001,L = 31.62,N=Nsim)
m, s = montecarlo_buckle(E,r_mean,r_std,L = 31.62,N=Nsim)

np.sum(critical_buckling_load < F)/Nsim *100
print(m,s)
plt.hist(critical_buckling_load, 1000)
```

```{code-cell} ipython3
np.sum(critical_buckling_load < F)/100000 * 100 #would fail
```

According to both the comparison methods above, if the rods had a length of 31.62 m the structure would fail. Verification: 31m x 0.1m would buckle since there is not enough weight holding it up.
